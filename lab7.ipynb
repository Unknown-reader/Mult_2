{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgNAhriAadJ0"
   },
   "source": [
    "## ЛР 7\n",
    "## (Медведев К.В. М8О-406Б-21)\n",
    "\n",
    "### Проведение исследований моделями семантической сегментации\n",
    "\n",
    "\n",
    "**План работ:**\n",
    "\n",
    "1. Выбор начальных условий:\n",
    "    - Выбрать набор данных для задачи классификации\n",
    "    - Выбрать метрики качества и обосновать их выбор\n",
    "\n",
    "2. Создание бейзлайна и оценка качества:\n",
    "    - Обучить модели segmentation_models.pytorch\n",
    "    - Оценить качество моделей по выбранным метрикам на выбранном наборе данных\n",
    "\n",
    "3. Улучшение бейзлайна:\n",
    "    - Сформулировать гипотезы (аугментации данных, подбор моделей, подбор гиперпараметров и т.д.)\n",
    "    - Проверить гипотезы\n",
    "    - Сформировать улучшенный бейзлайн по результатам проверки гипотез\n",
    "    - Обучить модели с улучшенным бейзлайном на выбранном наборе данных\n",
    "    - Оценить качество моделей с улучшенным бейзлайном по выбранным метрикам на выбранном наборе данных\n",
    "\n",
    "4. Имплементация алгоритма машинного обучения:\n",
    "    - Самостоятельно имплементировать модели машинного обучения\n",
    "    - Обучить имплементированные модели на выбранном наборе данных\n",
    "    - Оценить качество имплементированных моделей\n",
    "    - Добавить техники из улучшенного бейзлайна\n",
    "    - Обучить модели для выбранных наборов данных\n",
    "    - Оценить качество моделей\n",
    "5. \tСделать выводы\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPrEHAmVak3y"
   },
   "source": [
    "# 1. Выбор начальных условий\n",
    "\n",
    "Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1fAEKqfgyBM",
    "outputId": "cb2f35bf-f8fd-45e8-ade9-4d28995bf017"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузка датасета\n",
    "path = kagglehub.dataset_download(\"ihelon/football-player-segmentation\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UK3xf_HgyBS"
   },
   "outputs": [],
   "source": [
    "# Устанавливаем пути к фотографиям и аннотациям для них (Бинарным маскам)\n",
    "img_path = str(path) + \"/images/\"\n",
    "annotations_path = str(path) + \"/annotations/instances_default.json\"\n",
    "\n",
    "INPUT_SIZE = (1920, 1080)\n",
    "IMG_SIZE = 512\n",
    "N = 512\n",
    "\n",
    "with open(annotations_path) as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d55xP6wAbyUd"
   },
   "source": [
    "`COCO (Common Objects in Context)` — это формат аннотаций для датасетов, включающий информацию об изображениях, объектах на них, их сегментации, координатах и классах.\n",
    "\n",
    "Из JSON-файла в формате COCO извлекаются ID изображений и соответствующие им имена файлов, чтобы позже можно было легко сопоставить аннотацию с конкретным изображением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ghVL8RngyBS"
   },
   "outputs": [],
   "source": [
    "image_id_dict = {image['id']: image['file_name'] for image in annotations['images']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC2c7FOkcT6C"
   },
   "source": [
    "Далее создаются бинарные маски для каждого изображения, где закрашиваются области, соответствующие сегментациям объектов (игроков) из аннотаций COCO.\n",
    "\n",
    "Для каждого изображения маска сначала рисуется по полигонам в исходном размере, затем масштабируется до размера модели и сохраняется в массив `masks` как булевый массив с каналом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2KN2gXKQYJh"
   },
   "outputs": [],
   "source": [
    "masks = np.zeros((N, IMG_SIZE, IMG_SIZE), dtype=bool)\n",
    "\n",
    "# Создание масок по аннотациям\n",
    "for annotation in annotations['annotations']:\n",
    "    img_id = annotation['image_id']\n",
    "\n",
    "    mask = Image.new('1', INPUT_SIZE)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    for segmentation in annotation['segmentation']:\n",
    "        polygon = np.array(segmentation).reshape((-1, 2))\n",
    "        mask_draw.polygon([tuple(point) for point in polygon], fill=1)\n",
    "\n",
    "    bool_array = np.array(mask.resize((IMG_SIZE, IMG_SIZE))) > 0\n",
    "    masks[img_id - 1] = masks[img_id - 1] | bool_array\n",
    "\n",
    "masks = masks.reshape(N, IMG_SIZE, IMG_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp89A_bbed4P"
   },
   "source": [
    "На изображениях отображаются футбольные фотографии, поверх которых полупрозрачно наложены маски объектов (игроков), выделенные фиолетовым цветом. Это позволяет визуально оценить качество сегментации — какие области изображения модель должна распознавать как объекты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "kxSF9GKGdMI5",
    "outputId": "43f0ba47-00e3-4ae3-c930-06c0e914da5b"
   },
   "outputs": [],
   "source": [
    "def visualize_masks_overlay(img_dir, masks, image_id_dict, num_images=5, alpha=0.5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    shown = 0\n",
    "    for idx, (img_id, file_name) in enumerate(image_id_dict.items()):\n",
    "        if shown >= num_images:\n",
    "            break\n",
    "        img_path_full = os.path.join(img_dir, file_name)\n",
    "\n",
    "        if not os.path.exists(img_path_full):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path_full).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        mask = masks[img_id - 1].squeeze()\n",
    "        if mask.max() == 0:\n",
    "            continue\n",
    "\n",
    "        red_mask = np.zeros_like(img_np, dtype=np.uint8)\n",
    "        red_mask[mask] = [255, 0, 255]  # Фиолетовый цвет для маски\n",
    "\n",
    "        blended = img_np.copy()\n",
    "        blended[mask] = (alpha * red_mask[mask] + (1 - alpha) * img_np[mask]).astype(np.uint8)\n",
    "\n",
    "        plt.subplot(1, num_images, shown + 1)\n",
    "        plt.imshow(blended)\n",
    "        plt.title(f\"Фото {img_id}\")\n",
    "        plt.axis('off')\n",
    "        shown += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_masks_overlay(img_path, masks, image_id_dict, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6xF49POfTEi"
   },
   "source": [
    "\n",
    "Для оценки качества сегментации объектов на изображениях использованы две метрики:\n",
    "- **Intersection over Union (IoU)**\n",
    "- **Коэффициент Dice**.\n",
    "\n",
    "**IoU** измеряет долю пересечения предсказанной маски и истинной маски относительно их объединения:\n",
    "\n",
    "$$\n",
    "\\text{IoU} = \\frac{|P \\cap G|}{|P \\cup G|} = \\frac{\\text{intersection}}{\\text{union}}\n",
    "$$\n",
    "\n",
    "где:\n",
    "- \\( P \\) — бинарная предсказанная маска,\n",
    "- \\( G \\) — бинарная истинная маска.\n",
    "\n",
    "**Dice-мера** фокусируется на перекрытии между предсказанием и истинной маской и более чувствительна к небольшим объектам:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 |P \\cap G|}{|P| + |G|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R51GvjgKgRyS"
   },
   "outputs": [],
   "source": [
    "# Расчет Intersection over Union (IoU)\n",
    "def calculate_iou(preds, masks, threshold=0.5):\n",
    "    preds = (preds > threshold).float()\n",
    "    masks = masks.float()\n",
    "\n",
    "    intersection = torch.sum(preds * masks)  # Пересечение\n",
    "    union = torch.sum(preds) + torch.sum(masks) - intersection  # Объединение\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "# Расчет коэффициента Dice\n",
    "def calculate_dice(preds, masks, threshold=0.5):\n",
    "    preds = (preds > threshold).float()\n",
    "    masks = masks.float()\n",
    "\n",
    "    intersection = torch.sum(preds * masks)\n",
    "    dice = (2 * intersection) / (torch.sum(preds) + torch.sum(masks))\n",
    "\n",
    "    if torch.sum(preds) + torch.sum(masks) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApFey1YFg4oo"
   },
   "source": [
    "# 2. Создание бейзлайна и оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RbFbclOyVMO"
   },
   "source": [
    "Реализация обучения модели сегментации изображений с использованием PyTorch и PyTorch Lightning. Используется архитектура U-Net с энкодером ResNet34, предварительно обученным на ImageNet. Данные загружаются из кастомного датасета `FootballDataset`, где изображения и соответствующие маски сегментации преобразуются и подаются в DataLoader для пакетной обработки.\n",
    "\n",
    "Модель обучается с функцией потерь Dice Loss, оптимизатором Adam и метриками качества сегментации — метрикой Jaccard (IoU) и коэффициентом Dice. Обучение и валидация проходят с логированием значений потерь и метрик.\n",
    "\n",
    "**U-Net** - архитектура нейронной сети для сегментации изображений, состоящая из симметричного энкодера (сжатие признаков) и декодера (восстановление пространственной информации) с пропускными связями (skip connections).\n",
    "\n",
    "**Dice Loss** - функция потерь, основанная на коэффициенте Dice, измеряет степень совпадения между предсказанной маской и истинной маской. Полезна для задач сегментации с несбалансированными классами.\n",
    "\n",
    "Шаги выполнения:\n",
    "1. Подготовка и разбиение данных на обучающую и валидационную выборки.\n",
    "2. Определение кастомного датасета `FootballDataset` для загрузки изображений и масок.\n",
    "3. Создание модели U-Net с ResNet34 энкодером.\n",
    "4. Объявление функции потерь и метрик (Dice Loss, IoU, Dice Coefficient).\n",
    "5. Обучение модели с логированием метрик и потерь.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761,
     "referenced_widgets": [
      "12ae7c5796a349e291298d5818896643",
      "fad4c674dbe1428d9b32d94063c0dcbc",
      "58e54a65c03f4ff6890385ae3d44fb12",
      "b189db5ab77d4eae80cf904a893eb53a",
      "87277c3cd391410e9c20cab52d5d9ec9",
      "a7efe1325cd842dfb24124b9b486686c",
      "699907ab78a8475b87a35e0333b6f6b2",
      "296c7ec0a1c34b79a160b524617224ea",
      "ae3f8dbdda7349019fe1febdd559841b",
      "479f65571bd441109f008c440411b171",
      "970c7fded97240ca9e626c674890ba64",
      "12a8490c113b46e4bb652918e00a890b",
      "722b8740e11c4bc7add9859cf898652b",
      "24542c4041e840b19a8c745acbc49e04",
      "db851a05fcb64936973e013c0f810151",
      "232bdfe2c057468db7f7e72828b3e3cd",
      "05b21862e7f8445e8d88d0c3687db363",
      "8e4f1f757c5b44cb9115ea593e82a03f",
      "f67a4ef697794a918f90f05336652538",
      "2fbf9c29e99546a8ac577eca77c8e78d",
      "6a0bed9541aa4140a58de3e36411faa2",
      "272d0fb84ec344c8905df3260ee51bf2",
      "53e53e97cfdb4bac8f38dc64cf83a704",
      "605b7126264344319714cf65a8ccbfc5",
      "72dbacdf923e4415a1e0f5f004bc646b",
      "09ab1cef53364ab08c8e73fda1e21881",
      "ee49434e83f64e128bfbccbf1f08b295",
      "8f98aa164a2a4ac78b370061747d935f",
      "fb3b839a73004b9780c494d8553f8f0b",
      "179eb296c1e14450806dd2303d85d4d5",
      "55236901f9054684b061a767e479e9b9",
      "3c2ea6d19ab042939da1c250fdb1fc56",
      "1db190afe88e43018a5859646d5f5437",
      "bd1bbe7ad6f543cbb46c5111836cd483",
      "2c7126790ef3479aaa3fa6887e9179c3",
      "4596c602fff24e32a0757738cbce4a33",
      "22558350e5bb46eeb4d95d55bdb84530",
      "1cfa329fe7d54f8d85ab4bbc0e9a4c78",
      "cb5ebe0a414e4156b07c9f7972cf8a99",
      "2b2dc4f3bb254d31b3fcf588b4f7fdb7",
      "012039551c174affaf5866253f722f2c",
      "db1ebe4e9141425a97e8ba02a9b8f0a9",
      "6195f927a0924ca1800c781031d6b971",
      "de98344c88f7483c8e1f10a56ee3b038",
      "ced8dc92249e4db78a35ceb91f6f27e9",
      "a7ddf3eb55f8439aa09a5c5c45ad686e",
      "faa79dabcbe542419ad367c7af0fcae6",
      "1d28916d20f4475c996ee0d9cb47f15c",
      "5498cc7ddb52412a830ee6f7a75dbb59",
      "cff74d56ad8c47daaed640f8e276a46c",
      "8cf317590bb642a8a1569d8dc84bbb61",
      "807347e23b614bd6a812bdfcf1cb4bbb",
      "c5e217bcee2b4bcb9b8a7a139d281f39",
      "f58cccd091bf4a5d9881063525b89eb3",
      "8fe15584fcda4ec1b63cab798aa44090",
      "b6e46a36313c46548f1fed66432d80e2",
      "bc620655754b41949621c28c7763f570",
      "9a6da6fa41fe4bc89a44d4a1344acd73",
      "1d3be89dee704a73ae21472adbd20fa1",
      "a31624c385f845e881440c9a61c0cea9",
      "8b642b8b89be4f4492da1551aac66253",
      "68a608646c394db28319b29e9df698ed",
      "1d1884fda2464b7b8a3d6f28978cc99c",
      "41cbb17504154876a34c02d2e3ed1c08",
      "a9c8c095908344e6a6145dfd435e3592",
      "6c4b3d1714da41cb90dff15ec9b7bb65",
      "e2859340a78243e6babdf16e44439399",
      "2a22823b969b45c39dc50be25d72ca01",
      "796f2b04e9254db5b9ccfc540abc5ca0",
      "81ab3c08ba9145fdb1fc5ac915b9a9ac",
      "b589f40925304f78bdc7c118774c9ae3",
      "0cfc83d0042c4eb59b424bb98f7bb576",
      "b4411f822ddf4ba8b9d6ff5c449611b4",
      "df65ca2452ae4cc49a3667572b82643a",
      "081948d48494454b923bb87a56233aad",
      "044f7f48191345adac1fef9b886b0747",
      "e0ba50fd050f4e0f9c5e6575cc682710"
     ]
    },
    "id": "m2e01DMTmbbu",
    "outputId": "9d843ac6-1dd5-4689-847a-d4bf5d0272ef"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchmetrics.classification import JaccardIndex  # IoU метрика\n",
    "\n",
    "# Установка параметров\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Датасет\n",
    "class FootballDataset(Dataset):\n",
    "    def __init__(self, image_ids, masks, img_dir, transform=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.masks = masks\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_name = image_id_dict[img_id]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = self.masks[img_id - 1]\n",
    "\n",
    "        image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "        mask = Image.fromarray(mask * 255).resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        mask = np.array(mask) / 255\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "all_ids = list(image_id_dict.keys())\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = FootballDataset(train_ids, masks, img_path, transform=transform)\n",
    "val_dataset = FootballDataset(val_ids, masks, img_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Модель\n",
    "class UNetLightning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None\n",
    "        )\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        self.metric_iou = JaccardIndex(task=\"binary\").to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        iou = self.metric_iou(preds.int(), y.int())\n",
    "        self.log('train_iou', iou, prog_bar=True)\n",
    "\n",
    "        dice = self.dice_coeff(preds, y)\n",
    "        self.log('train_dice', dice, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        iou = self.metric_iou(preds.int(), y.int())\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        dice = self.dice_coeff(preds, y)\n",
    "        self.log('val_dice', dice, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_coeff(preds, targets, smooth=1e-6):\n",
    "        preds = preds.float()\n",
    "        targets = targets.float()\n",
    "        intersection = (preds * targets).sum(dim=[1, 2, 3])\n",
    "        union = preds.sum(dim=[1, 2, 3]) + targets.sum(dim=[1, 2, 3])\n",
    "        dice = (2 * intersection + smooth) / (union + smooth)\n",
    "        return dice.mean()\n",
    "\n",
    "# Обучение модели\n",
    "model = UNetLightning()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    logger=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Bq41N8kxHNt"
   },
   "source": [
    "# 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkR__Qw-z0A8"
   },
   "source": [
    "В улучшенном бейзлайне была добавлена аугментация изображений с помощью библиотеки Albumentations, включающая такие преобразования, как случайное горизонтальное отражение, изменение яркости и контраста, сдвиги, масштабирование и повороты, а также нормализация под статистику ImageNet и преобразование в тензоры. Эти аугментации применяются только к обучающему набору и синхронизированы с соответствующими масками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "b6c90deecc9d484686301c91124fd367",
      "3d1e9b27aaf74feaa1e7e902442c16de",
      "ac786ece1eb44229bd563f5116781523",
      "f27d16f309cd4b2d9d3762e42d91447d",
      "5c999b318b374bf1a957e482ec790ac3",
      "c1c286c6aa1c4a6eb6b9513556b0529d",
      "b5ed33f53552472ea32bb747573cfc23",
      "acad0baff2f64664a2a9a1c832f4d430",
      "22f46e3426e14be8a43c633d638bf8a8",
      "bb46fedc985744d088d5c468ef1ba744",
      "fcd63d869a674e6a84ffdf8fad155777",
      "4e452151d5464823baa1a7f4ceef7aae",
      "2fdaddda072a433d800be77ab492104f",
      "5712014f715f4eb38270b13397ae3afd",
      "cff8a144c6184bc7bc06190f7e6670c7",
      "99ac1ecb6fd8454299b292c978f639d8",
      "b31b8a5799fe448ea3b22b2aca80c675",
      "3fa7615985fc4a7fba44a9b4eaeb99e4",
      "9aee09d7c2684855b23e7699b3384669",
      "ff65cf9ff494412c95e40adf3336f270",
      "c764e1381a5248c98f934991a66bce90",
      "2b2bd5953f2e4f12a450645981bca3e0",
      "be1acf86c466435cbe93140993f60cb8",
      "f8b6cac2c6ad4367a49cbadd332216f1",
      "a074b9062db043c7a428df56468148bf",
      "838625d4b0b04ce8971269728ab7039f",
      "e227195f61754f3d82d270dbe0a37e01",
      "479c72bb5d774075bb08ee4b621886b2",
      "26772afe5afe44aaa88563d1c6c2b314",
      "f70b88fdc6484e3eab3327702569ab0e",
      "d644eaea5b2a4d58ae07593c05cfe186",
      "4b257f2ef08041e7a4bd52ad25d50a8d",
      "6a58c99358df48fb83179cd7c8dcce1a",
      "4a91884103e5451c8469de84d32f9914",
      "b4c124fa0cc74fdf9941ac433a7b16cb",
      "98c00c65ba6145afa971ced929d537d1",
      "16355e746ddc4bfb9aad53f5643b6e33",
      "dfd82c4e59134f17b8caf597e1638c9f",
      "f64bc2270c94468db680b2636e759844",
      "e8b59530dbb742f0804dc7404a5fe4ff",
      "1dfba99441ec44319a34a1862e64a80a",
      "c2cf7e9a6561487c9da9bf7fa7bfe136",
      "573184a987924995838dbfb76de3c713",
      "90763309482d4aecb00981cb0b7eed1c",
      "f0e3c36a9df2489581829f5c44de03e4",
      "f566e11e562144bbaf9611c9577d8525",
      "30400682ded44d339307a6e1c8209c05",
      "6871cd4101814cd29a5f2ff99eed6bdb",
      "e84b8b9529c540988ce579a6db62064b",
      "cd42333e8d5249e5b2168826089d6fcb",
      "96abc08473964731828bb3f9594ba652",
      "0873b0abc716417886de3d06c31040cf",
      "14f6a02674144641a8bbc6be69264010",
      "7f8218bbe95f49fb9055546b1396a249",
      "874c8c6ded6e4a689836383a3c489bde",
      "e77301167c7742b986c46c5e24470079",
      "d2f45ff7d223494086ff10e181d9b606",
      "b00871ab992840019820cd52fddc26bf",
      "a628d0425cb94290a9b36d2be372bf32",
      "4238d06d1c144d16a7e4229584de0fa8",
      "d2094eef3f8c4e7e8001112e97ae84a8",
      "ec52080afbd24d31a1e6bb87a27fdaa9",
      "807fa6fd0164444e976db7d82ba0f297",
      "b1a899fd4b4a4e1ab963761a4fdf685a",
      "6e76ac08a23d4e09bd4c2ba538e46de9",
      "470b9f3926a84efea395b902a0d77a54",
      "cf9c3d97de6441e0ad543029898a6843",
      "a7a11d762dac402fa23d6d11ac0188cc",
      "30ab6bc96de94fffbf8548259f83a50a",
      "4541938a4844423898220f806f0d5b01",
      "8c52f7709a794ad3ab40c73974883ee7",
      "1d7b4a67d7ec428eaa774382c9d1bb52",
      "1fa8f4d8e9a74cd18f85046cea1c78e1",
      "9a59bcc83e504656a5335d450b6132e6",
      "609eb77d5232443b8c7994053e86c834",
      "1bbaba19db87416ebb32bbc826d761d8",
      "f9377ecc6a414b4a8e8e13ef6e6a5f59"
     ]
    },
    "id": "lsEaNvE1xS3b",
    "outputId": "7a672f71-a7f5-45b3-d86a-bc4996f8336c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import JaccardIndex\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import kagglehub\n",
    "\n",
    "# Загрузка датасета\n",
    "path = kagglehub.dataset_download(\"ihelon/football-player-segmentation\")\n",
    "img_path = str(path) + \"/images/\"\n",
    "annotations_path = str(path) + \"/annotations/instances_default.json\"\n",
    "\n",
    "INPUT_SIZE = (1920, 1080)\n",
    "IMG_SIZE = 512\n",
    "N = 512\n",
    "\n",
    "with open(annotations_path) as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "image_id_dict = {image['id']: image['file_name'] for image in annotations['images']}\n",
    "\n",
    "masks = np.zeros((N, IMG_SIZE, IMG_SIZE), dtype=bool)\n",
    "\n",
    "for annotation in annotations['annotations']:\n",
    "    img_id = annotation['image_id']\n",
    "    mask = Image.new('1', INPUT_SIZE)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    for segmentation in annotation['segmentation']:\n",
    "        polygon = np.array(segmentation).reshape((-1, 2))\n",
    "        mask_draw.polygon([tuple(point) for point in polygon], fill=1)\n",
    "\n",
    "    bool_array = np.array(mask.resize((IMG_SIZE, IMG_SIZE))) > 0\n",
    "    masks[img_id - 1] = masks[img_id - 1] | bool_array\n",
    "\n",
    "masks = masks.reshape(N, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "# Аугментации\n",
    "IMAGE_SIZE = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "class FootballDataset(Dataset):\n",
    "    def __init__(self, image_ids, masks, img_dir, transform=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.masks = masks\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_name = image_id_dict[img_id]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        image = np.array(image)\n",
    "        mask = self.masks[img_id - 1].squeeze().astype('float32')\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask'].unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Разделение данных\n",
    "all_ids = list(image_id_dict.keys())\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = FootballDataset(train_ids, masks, img_path, transform=train_transform)\n",
    "val_dataset = FootballDataset(val_ids, masks, img_path, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "# Модель Lightning\n",
    "class UNetLightning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None\n",
    "        )\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        self.metric_iou = JaccardIndex(task=\"binary\").to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        iou = self.metric_iou(preds.int(), y.int())\n",
    "        self.log('train_iou', iou, prog_bar=True)\n",
    "\n",
    "        dice = self.dice_coeff(preds, y)\n",
    "        self.log('train_dice', dice, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        iou = self.metric_iou(preds.int(), y.int())\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        dice = self.dice_coeff(preds, y)\n",
    "        self.log('val_dice', dice, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_coeff(preds, targets, smooth=1e-6):\n",
    "        preds = preds.float()\n",
    "        targets = targets.float()\n",
    "        intersection = (preds * targets).sum(dim=[1, 2, 3])\n",
    "        union = preds.sum(dim=[1, 2, 3]) + targets.sum(dim=[1, 2, 3])\n",
    "        dice = (2 * intersection + smooth) / (union + smooth)\n",
    "        return dice.mean()\n",
    "\n",
    "# Обучение модели\n",
    "model = UNetLightning()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    logger=True,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVkhhd1Tw8cy"
   },
   "source": [
    "# 4. Имплементация алгоритма машинного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4TcNR4zQnXS"
   },
   "outputs": [],
   "source": [
    "images = np.zeros((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "for img_id, img_filename in image_id_dict.items():\n",
    "    img = Image.open(os.path.join(img_path, img_filename))\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "    images[img_id - 1] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufsIgQkHRw5c"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Contracting path (Encoder)\n",
    "        self.encoder1 = self.conv_block(3, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        self.encoder5 = self.conv_block(512, 1024)  # Additional layer\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(1024, 2048)\n",
    "\n",
    "        # Expanding path (Decoder)\n",
    "        self.upconv5 = nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2)\n",
    "        self.decoder5 = self.conv_block(2048, 1024)\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.conv_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.conv_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.conv_block(128, 64)\n",
    "\n",
    "        # Final output layer (binary segmentation)\n",
    "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (contracting path)\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2))\n",
    "        enc5 = self.encoder5(F.max_pool2d(enc4, kernel_size=2))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc5, kernel_size=2))\n",
    "\n",
    "        # Decoder (expanding path)\n",
    "        dec5 = self.upconv5(bottleneck)\n",
    "        dec5 = torch.cat((enc5, dec5), dim=1)\n",
    "        dec5 = self.decoder5(dec5)\n",
    "        dec4 = self.upconv4(dec5)\n",
    "        dec4 = torch.cat((enc4, dec4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        # Final output\n",
    "        return torch.sigmoid(self.conv_last(dec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7MfQRi5RyJ6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZjGS-G8R3J-"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images  # Image data\n",
    "        self.masks = masks    # Mask data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Fetch the image\n",
    "        mask = self.masks[idx]    # Fetch the corresponding mask\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.from_numpy(image).to(torch.float32) / 255.0  # Normalize image to [0, 1]\n",
    "        mask = torch.from_numpy(mask).to(torch.float32)\n",
    "\n",
    "        # Change dimension order from [H, W, C] to [C, H, W]\n",
    "        image = image.permute(2, 0, 1)\n",
    "        mask = mask.permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = SegmentationDataset(images=train_images, masks=train_masks)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "val_dataset = SegmentationDataset(images=val_images, masks=val_masks)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yz1qjM-0R8GD",
    "outputId": "9ebc3b8a-cdb5-47d2-d81a-6947db603c18"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "iou_list = []\n",
    "dice_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training phase\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()  # Convert masks to float\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, masks)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iou_total = 0.0\n",
    "    dice_total = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate IoU and Dice Coefficient\n",
    "            iou = calculate_iou(outputs, masks)\n",
    "            dice = calculate_dice(outputs, masks)\n",
    "\n",
    "            iou_total += iou\n",
    "            dice_total += dice\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_iou = iou_total / len(val_loader)\n",
    "    avg_dice = dice_total / len(val_loader)\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    iou_list.append(avg_iou)\n",
    "    dice_list.append(avg_dice)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"IoU: {avg_iou:.4f}, Dice Coefficient: {avg_dice:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
